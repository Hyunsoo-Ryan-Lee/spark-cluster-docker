{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec208108-702e-4753-8b80-6d637992d8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@295bda8b\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    ".appName(\"sparksql\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5184965a-ab26-4243-8e8e-bdac4cf3eeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars: org.apache.spark.sql.DataFrame = [Acceleration: double, Cylinders: bigint ... 7 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cars = spark.read.option(\"inferSchema\", \"true\").json(\"./dataset/cars.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cde298-316f-4c79-ab49-6754444cdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Name|Origin|\n",
      "+--------------------+------+\n",
      "|chevrolet chevell...|   USA|\n",
      "|   buick skylark 320|   USA|\n",
      "|  plymouth satellite|   USA|\n",
      "+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars.createOrReplaceTempView(\"cars\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT Name, Origin FROM cars WHERE Origin = 'USA'\n",
    "    \"\"\"\n",
    "    ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c07402-b0fb-4f5a-98e2-31c40721eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9930ae61-3513-4c6d-844a-adf06d5ff246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| midnight|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create database midnight\")\n",
    "\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcfdf3e-43a3-4e4d-8017-168c35345a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@295bda8b\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    ".appName(\"sparksql\")\n",
    ".config(\"spark.sql.warehouse.dir\", \"manual-warehouse\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4ccfb8-8b1c-458b-8557-951d537e6554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     temp|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create database temp\")\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733811ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException",
     "evalue": " [SCHEMA_NOT_FOUND] The schema `auto` cannot be found. Verify the spelling and correctness of the schema and catalog.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `auto` cannot be found. Verify the spelling and correctness of the schema and catalog.",
      "If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.",
      "To tolerate the error on drop use DROP SCHEMA IF EXISTS.",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:250)",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.setCurrentDatabase(SessionCatalog.scala:338)",
      "  at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentNamespace(CatalogManager.scala:109)",
      "  at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.$anonfun$run$2(SetCatalogAndNamespaceExec.scala:36)",
      "  at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.$anonfun$run$2$adapted(SetCatalogAndNamespaceExec.scala:36)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.sql.execution.datasources.v2.SetCatalogAndNamespaceExec.run(SetCatalogAndNamespaceExec.scala:36)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"use auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d248afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carsFilter: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Acceleration: double, Cylinders: bigint ... 7 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val carsFilter = cars.filter(col(\"Origin\") === \"USA\")\n",
    "\n",
    "carsFilter.coalesce(1).write.mode(SaveMode.Overwrite).saveAsTable(\"usacar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdda11ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb2c7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|                Name|Origin|Weight_in_lbs|      Year|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            15.0|   buick skylark 320|   USA|         3693|1970-01-01|\n",
      "|        11.0|        8|       318.0|       150|            18.0|  plymouth satellite|   USA|         3436|1970-01-01|\n",
      "|        12.0|        8|       304.0|       150|            16.0|       amc rebel sst|   USA|         3433|1970-01-01|\n",
      "|        10.5|        8|       302.0|       140|            17.0|         ford torino|   USA|         3449|1970-01-01|\n",
      "|        10.0|        8|       429.0|       198|            15.0|    ford galaxie 500|   USA|         4341|1970-01-01|\n",
      "|         9.0|        8|       454.0|       220|            14.0|    chevrolet impala|   USA|         4354|1970-01-01|\n",
      "|         8.5|        8|       440.0|       215|            14.0|   plymouth fury iii|   USA|         4312|1970-01-01|\n",
      "|        10.0|        8|       455.0|       225|            14.0|    pontiac catalina|   USA|         4425|1970-01-01|\n",
      "|         8.5|        8|       390.0|       190|            15.0|  amc ambassador dpl|   USA|         3850|1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from usacar limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b92aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@226cfc2\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder\n",
    ".appName(\"sparksql\")\n",
    ".config(\"spark.sql.warehouse.dir\", \"/workspace/spark/manual-warehouse\")\n",
    ".config(\"spark.ui.port\", \"0\")\n",
    ".enableHiveSupport\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeae5a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark\n",
      "spark-3.5.0-bin-hadoop3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
       "import sys.process._\n",
       "res2: Int = 0\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"ls /workspace\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9abd7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "26: error: not found: value %",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: not found: value %",
      "       %load_ext sql",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "1. sparark.sql.warehouse.dir=/workspace/spark/warehouse\n",
    "\n",
    "2. create database manual location '/workspace/spark/warehouse/manual.db';\n",
    "\n",
    "CREATE DATABASE temp\n",
    "SELECT current_database()\n",
    "USE temp\n",
    "SHOW tables\n",
    "DROP DATABASE temp CASCADE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8012430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0a730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
